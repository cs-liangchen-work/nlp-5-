#!/usr/bin/env python
# coding: utf-8

import gensim
import datetime
import numpy as np
import torch
import torch.nn.utils.rnn as rnn_utils
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, TensorDataset
from gensim.models import Word2Vec
from collections import Counter
import argparse
from sklearn.metrics import accuracy_score
import os

# ======================== 一些参数的初始化 ========================
parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=512)
parser.add_argument('--num_epoch', type=int, default=50)
parser.add_argument('--lr', type=float, default=1e-4)
parser.add_argument('--weight_decay', type=float, default=1e-7)
parser.add_argument('--ckp', type=str, default='./ckp/model_2.pt')  # 保存模型的参数.
parser.add_argument('--acc_min', type=float, default=0.0)
parser.add_argument('--nums_channels', type=int, default=50)
args = parser.parse_args()
acc_min = args.acc_min
device = torch.device('cpu')

# ======================== 数据预处理 ========================
# 读入训练集
with open('./train.txt', encoding='utf-8') as ftrain_feature:
    train_feature_line = [line.strip() for line in ftrain_feature.readlines()]
train_label_line = [1] * int(len(train_feature_line) / 2)
temp = [0] * int(len(train_feature_line) / 2)
# >>> [0]*5
# [0, 0, 0, 0, 0]
train_label_line.extend(temp)

# 读入验证集
with open('./valid.txt', encoding='utf-8') as ftest_feature:
    test_feature_line = [line.strip() for line in ftest_feature.readlines()]
test_label_line = [1] * int(len(test_feature_line) / 2)
temp = [0] * int(len(test_feature_line) / 2)
test_label_line.extend(temp)

# 用split分隔开存入列表
train_feature_line = [line.split(" ") for line in train_feature_line]
test_feature_line = [line.split(" ") for line in test_feature_line]

# ======================== 将GloVe处理成Word2Vec ========================

""" glove_file = './GloVe-master/vectors.txt'
tmp_file = './GloVe-master/word2vec_model.txt'
_ = glove2word2vec(glove_file, tmp_file)"""

# 获得单词字典
# model_word2vec = Word2Vec([['[UNK]'], ['[PAD]']]+train_feature_line, sg=1, min_count=1, size=128, window=5)
# model_word2vec.save('word2vec_model.txt')
# w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GloVe-master/word2vec_model.txt',binary=False, encoding='utf-8')
w2v_model = Word2Vec.load('word2vec_model.txt')
word2id = dict(zip(w2v_model.wv.index2word, range(len(w2v_model.wv.index2word))))  # word -> id
id2word = {idx: word for idx, word in enumerate(w2v_model.wv.index2word)}  # id -> word
unk = word2id['[UNK]']  # UNK:低频词
padding_value = word2id['[PAD]']  # PAD:填充词

# 获得数据序列
train_feature = [[word2id[word] if word in word2id else unk for word in line] for line in train_feature_line]
test_feature = [[word2id[word] if word in word2id else unk for word in line] for line in test_feature_line]


def get_dataset(dictionary, sample_features, sample_labels):
    sample_data = []
    for i in range(len(sample_features)):
        temp = []
        temp.append(torch.Tensor(sample_features[i]).long())
        temp.append(torch.Tensor([sample_labels[i]]).long())
        sample_data.append(temp)
    return sample_data


train_data = get_dataset(word2id, train_feature, train_label_line)
test_data = get_dataset(word2id, test_feature, test_label_line)
# 处理完成之后，我们就得到了列表1 列表1中的每一个元素是列表2，
# 列表2元素的第一个元素是由句子的词组成的列表，第二个元素是其对应的标签。

class TextCNN(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size, embeding_vector, kernel_sizes, num_channels):
        super().__init__()
        self.hidden_size = hidden_size
        # 不参与训练的与词向量相关的嵌入层。属于基本的套路。
        # 第三行设置成False，因此不参与训练，因为不想破坏已经训练好的词向量。
        self.embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)
        self.embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  # 使用预训练的词向量
        self.embedding.weight.requires_grad = False
        # 参与训练的嵌入层
        # constant_embedding没有见过？？？？？？？？？？？？？？？？？？？？
        self.constant_embedding = torch.nn.Embedding(num_embeddings=input_size, embedding_dim=hidden_size)
        self.constant_embedding.weight.data.copy_(torch.from_numpy(embeding_vector))  # 使用预训练的词向量
        self.dropout = torch.nn.Dropout(0.5)
        self.out_linear = torch.nn.Linear(sum(num_channels), output_size)
        self.pool = GlobalMaxPool1d()
        self.convs = torch.nn.ModuleList()
        # nn.ModuleList仅仅类似于pytho中的list类型，只是将一系列层装入列表，并没有实现forward()方法。
        for c, k in zip(num_channels, kernel_sizes):
            self.convs.append(torch.nn.Conv1d(in_channels=2 * hidden_size, out_channels=c, kernel_size=k))
            # torch.nn.Conv1d：一维卷积参数解释：
            # in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度    该问题中静态和动态的合并了，所以乘了2
            # 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积
            # 卷积核的尺寸
# 通过代码  我觉得模型中是不同的通道使用不同的核卷积的大小来提取特征。
    def forward(self, x):
        # 第一步实现的应该是论文中提到的‘CNN-multichannel’方法，两个词向量，一个是动态的，即训练过程中改变其参数
        # 一个是静态的，即训练过程中不改变其参数。      该代码中直接使用cat将两个向量拼接成了一个embedding
        embeddings = torch.cat((self.embedding(x), self.constant_embedding(x)), dim=2).permute(0, 2, 1)  # permute 是将张量的维度进行变换
        out = torch.cat([self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)
        out = self.out_linear(self.dropout(out))
        return out


class GlobalMaxPool1d(torch.nn.Module):
    def __init__(self):
        super().__init__()
# ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？    整个模型，卷积的过程都很不清楚。
    def forward(self, x):
        return F.max_pool1d(x, kernel_size=x.shape[2])


def collate_fn(sample_data):
    sample_data.sort(key=lambda data: len(data[0]), reverse=True)  # 倒序排序
    sample_features = []
    sample_labels = []
    for data in sample_data:
        sample_features.append(data[0])
        sample_labels.append(data[1])
    data_length = [len(data[0]) for data in sample_data]  # 取出所有data的长度
    sample_features = rnn_utils.pad_sequence(sample_features, batch_first=True, padding_value=padding_value)
    return sample_features, sample_labels, data_length


def test_evaluate(model, test_dataloader, batch_size):
    test_l, test_a, n = 0.0, 0.0, 0
    model.eval()
    with torch.no_grad():
        for data_x, data_y, _ in test_dataloader:
            label = torch.Tensor(data_y).long().to(device)
            out = model(data_x.to(device))
            prediction = out.argmax(dim=1)
            loss = loss_func(out, label)
            prediction = out.argmax(dim=1).data.cpu().numpy()
            label = label.data.cpu().numpy()
            test_l += loss.item()
            test_a += accuracy_score(label, prediction)
            n += 1
    return test_l / n, test_a / n


loss_func = torch.nn.CrossEntropyLoss()
embedding_matrix = w2v_model.wv.vectors
input_size = embedding_matrix.shape[0]  # 37125, 词典的大小
hidden_size = embedding_matrix.shape[1]  # 50, 隐藏层单元个数
# 卷积核相关的定义，这里应该注意一下子。*************
kernel_size = [3, 4, 5]
nums_channels = [args.nums_channels, args.nums_channels, args.nums_channels]
model = TextCNN(input_size, hidden_size, 2, embedding_matrix, kernel_size, nums_channels).to(device)
if os.path.exists(args.ckp):
    print("loading model......")
    model.load_state_dict(torch.load(args.ckp))
optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)  #

train_dataloader = DataLoader(train_data, args.batch_size, collate_fn=collate_fn, shuffle=True)
test_dataloader = DataLoader(test_data, args.batch_size, collate_fn=collate_fn, shuffle=True)

train_loss = []
train_accuracy = []
test_loss = []
test_accuracy = []
for epoch in range(args.num_epoch):
    model.train()
    train_l, train_a, n = 0.0, 0.0, 0
    start = datetime.datetime.now()
    for data_x, data_y, _ in train_dataloader:
        label = torch.Tensor(data_y).long().to(device)
        out = model(data_x.to(device))
        loss = loss_func(out, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        prediction = out.argmax(dim=1).data.cpu().numpy()
        label = label.data.cpu().numpy()
        train_l += loss.item()
        train_a += accuracy_score(label, prediction)
        n += 1
    # 训练集评价指标
    train_loss.append(train_l / n)
    train_accuracy.append(train_a / n)
    # 测试集评价指标
    test_l, test_a = test_evaluate(model, test_dataloader, args.batch_size)
    test_loss.append(test_l)
    test_accuracy.append(test_a)
    end = datetime.datetime.now()
    print('epoch %d, train_loss %f, train_accuracy %f, test_loss %f, test_accuracy %f, time %s' %
          (epoch + 1, train_loss[epoch], train_accuracy[epoch], test_loss[epoch], test_accuracy[epoch], end - start))
    if test_accuracy[epoch] > acc_min:
        acc_min = test_accuracy[epoch]
        f = open(args.ckp, 'w', encoding='utf-8')
        f.close()
        torch.save(model.state_dict(), args.ckp)
        print("save model...")
