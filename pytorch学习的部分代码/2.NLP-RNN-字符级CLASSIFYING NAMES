主要实现的是RNN最基础的网络
输入字母组成的单次，判断他是哪一种语言，需要用到序列的信息，因此使用了RNN。

而且好像不是用经典的RNN模型来实现的，是把输入和hidden的进行拼接了。


涉及到了画图，之后想学了可以学一学、


加了注释代码：
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os
import unicodedata
import string
import torch
import torch.nn as nn
# ======================================= 读取数据： ======================================= #
def findFiles(path): return glob.glob(path)
# 返回所有匹配的文件路径列表。
# print(findFiles('data/names/*.txt'))
# 获得data/names目录中，所有格式为*.txt的文件的列表。
all_letters = string.ascii_letters + " .,;'"
# string.ascii_letters 的 内容  abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ
n_letters = len(all_letters)

# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427
# 把没编码的字符串转化为ASCII编码格式的字符串。
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )

# print(unicodeToAscii('Ślusàrski'))
# 输出：Slusarski。

# Build the category_lines dictionary, a list of names per language
category_lines = {}  # 看清是一个字典。
all_categories = []

# Read a file and split into lines
def readLines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    # 通过 split('\n') 实现把每一行分开。
    return [unicodeToAscii(line) for line in lines]
    # 对每一行进行上述定义的unicodeToAscii，都为ASCII格式的字符串编码了。
for filename in findFiles('data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    # os.path.splitext() 将文件名和扩展名分开     【0】  只有文件名了。
    all_categories.append(category)
    lines = readLines(filename)
    category_lines[category] = lines
# 经过处理之后，category_lines作为一个字典， category_lines【chinese】的内容就是对应的列表，
# 列表中的内容：  二维张量，  【句子1字符串，句子2字符串，。。。】
# 因为该预料每一行给的都是单个的词， 【字符串1，字符串2，。。。】
n_categories = len(all_categories)

# 数据变为tensor的格式：
# Find letter index from all_letters, e.g. "a" = 0
def letterToIndex(letter):
    return all_letters.find(letter)
# Just for demonstration, turn a letter into a <1 x n_letters> Tensor
def letterToTensor(letter):
    # 相当于转化为onehot类型的张量。
    tensor = torch.zeros(1, n_letters)
    # tensor = torch.zeros(1, 5)        tensor([[0., 0., 0., 0., 0.]])
    tensor[0][letterToIndex(letter)] = 1
    return tensor
# Turn a line into a <line_length x 1 x n_letters>,
# or an array of one-hot letter vectors
def lineToTensor(line):
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        tensor[li][0][letterToIndex(letter)] = 1
    return tensor
# print(letterToTensor('J'))
# tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
#          0., 0., 0.]])
# print(lineToTensor('Jones').size())
# torch.Size([5, 1, 57])

# 暂时不理解为什么有多余的一维呢？


# ======================================= 定义网络 ======================================= #
class RNN(nn.Module):  #我咋感觉这个RNN 和一般的不一样呢？？？？
    def __init__(self, input_size, hidden_size, output_size):  # 57 128 18
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)
        # softmax：相当于求概率分布，【0.1】
        # LogSoftmax：对上述再一次log，因此为负值。

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        # 将input和hidden按照第2个维度拼接。
        hidden = self.i2h(combined)
        output = self.i2o(combined)
        output = self.softmax(output)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
# print(n_letters, n_hidden, n_categories)   57 128 18
rnn = RNN(n_letters, n_hidden, n_categories)
# ======================================= 网络测试 ======================================= #
'''
# 一个简单的测试用例。
input = letterToTensor('A')
hidden =torch.zeros(1, n_hidden)

output, next_hidden = rnn(input, hidden)

input = lineToTensor('Albert')
hidden = torch.zeros(1, n_hidden)
# print(input[0].shape)   torch.Size([1, 57])
output, next_hidden = rnn(input[0], hidden)
# print(output)
# print(output.shape)  torch.Size([1, 18])
# print(next_hidden.shape)  torch.Size([1, 128])

def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    # 因为输出是torch.Size([1, 18])的，因此是提取第一维度的最大值。
    # top_n, top_i    值   下标
    category_i = top_i[0].item()  # 提取张量的值
    return all_categories[category_i], category_i

# print(categoryFromOutput(output))
# ('Greek', 7)
'''

# ======================================= 建立数据集的方法 ======================================= #
import random
def randomChoice(l):
    return l[random.randint(0, len(l) - 1)]

def randomTrainingExample():
    category = randomChoice(all_categories)
    line = randomChoice(category_lines[category])
    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)
    line_tensor = lineToTensor(line)
    return category, line, category_tensor, line_tensor
    # 是哪一类，  该类中的一个句子， 变成张量， 句子变成张量。

for i in range(10):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    # print('category =', category, '/ line =', line)
    # Arabic Essa   第一个是语言的一种，第二个是该类语言的一个词。
    # print(category_tensor.shape)  torch.Size([1])
    # print(line_tensor.shape)  torch.Size([4, 1, 57])   因为这个单词有4个词

# ======================================= 训练 ======================================= #
criterion = nn.NLLLoss()
learning_rate = 0.005

def train(category_tensor, line_tensor):
    hidden = rnn.initHidden()
    rnn.zero_grad()
    # 用这个循环来是实现循环神经网络 信息的不断向后传递。   因此输入字符串后的输出还是一个预测的结果
    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)
    loss = criterion(output, category_tensor)
    loss.backward()
    # Add parameters' gradients to their values, multiplied by learning rate
    # 应该可以使用那个step函数吧！
    for p in rnn.parameters():
        p.data.add_(p.grad.data, alpha=-learning_rate)

    return output, loss.item()


import time
import math

n_iters = 100000
print_every = 5000
plot_every = 1000
# Keep track of losses for plotting
current_loss = 0
all_losses = []

def timeSince(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

start = time.time()

def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_categories[category_i], category_i


for iter in range(1, n_iters + 1):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    output, loss = train(category_tensor, line_tensor)
    current_loss += loss

    # Print iter number, loss, name and guess
    if iter % print_every == 0:
        guess, guess_i = categoryFromOutput(output)
        correct = '✓' if guess == category else '✗ (%s)' % category
        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))

    # Add current loss avg to list of losses
    if iter % plot_every == 0:
        all_losses.append(current_loss / plot_every)
        current_loss = 0
'''

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

plt.figure()
plt.plot(all_losses)

# Keep track of correct guesses in a confusion matrix
confusion = torch.zeros(n_categories, n_categories)
n_confusion = 10000

# Just return an output given a line
def evaluate(line_tensor):
    hidden = rnn.initHidden()

    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)

    return output

# Go through a bunch of examples and record which are correctly guessed
for i in range(n_confusion):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    output = evaluate(line_tensor)
    guess, guess_i = categoryFromOutput(output)
    category_i = all_categories.index(category)
    confusion[category_i][guess_i] += 1

# Normalize by dividing every row by its sum
for i in range(n_categories):
    confusion[i] = confusion[i] / confusion[i].sum()

# Set up plot
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(confusion.numpy())
fig.colorbar(cax)

# Set up axes
ax.set_xticklabels([''] + all_categories, rotation=90)
ax.set_yticklabels([''] + all_categories)

# Force label at every tick
ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

# sphinx_gallery_thumbnail_number = 2
plt.show()

def predict(input_line, n_predictions=3):
    print('\n> %s' % input_line)
    with torch.no_grad():
        output = evaluate(lineToTensor(input_line))

        # Get top N categories
        topv, topi = output.topk(n_predictions, 1, True)
        predictions = []

        for i in range(n_predictions):
            value = topv[0][i].item()
            category_index = topi[0][i].item()
            print('(%.2f) %s' % (value, all_categories[category_index]))
            predictions.append([value, all_categories[category_index]])

predict('Dovesky')
predict('Jackson')
predict('Satoshi')
'''



# =======================================  ======================================= #

# =======================================  ======================================= #
















再来一遍：

'''
字符级别的，真的懒的自己写一遍了。
知识深入理解了网路的运行过程。
'''
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os
import unicodedata
import string
import torch
import torch.nn as nn
import time
import math
import random
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# ======================================= 读取数据： ======================================= #
all_letters = string.ascii_letters + " .,;'"
# abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ + " .,;'"
n_letters = len(all_letters)

# 文件读取
def findFiles(path): return glob.glob(path)

# ASCII编码
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )

# Read a file and split into lines
def readLines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    return [unicodeToAscii(line) for line in lines]


category_lines = {}
all_categories = []
for filename in findFiles('data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    lines = readLines(filename)
    category_lines[category] = lines
n_categories = len(all_categories)

# eg. ’a‘=0
def letterToIndex(letter):
    return all_letters.find(letter)

# one-hot
def letterToTensor(letter):
    tensor = torch.zeros(1, n_letters)
    # tensor = torch.zeros(1, 5)        tensor([[0., 0., 0., 0., 0.]])
    tensor[0][letterToIndex(letter)] = 1
    return tensor

def lineToTensor(line):
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        tensor[li][0][letterToIndex(letter)] = 1
    return tensor
# print(lineToTensor('Jones').size())
# torch.Size([5, 1, 57])

# ======================================= 定义网络 ======================================= #
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.Linear(input_size + hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        hidden = self.i2h(combined)
        output = self.i2o(combined)
        output = self.softmax(output)
        return output, hidden

    def initHidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
# print(n_letters, n_hidden, n_categories)   57 128 18
rnn = RNN(n_letters, n_hidden, n_categories)

# ======================================= 简单的测试用例 ======================================= #
'''
# 一个
input = letterToTensor('A')
hidden =torch.zeros(1, n_hidden)
# print(input.shape)          torch.Size([1, 57])
# print(hidden.shape)         torch.Size([1, 128])
output, next_hidden = rnn(input, hidden)
# print(output.shape)         torch.Size([1, 18])
# print(next_hidden.shape)    torch.Size([1, 128])
'''

'''
input = lineToTensor('Albert')
hidden = torch.zeros(1, n_hidden)
# print(input[0].shape)   torch.Size([1, 57])
output, next_hidden = rnn(input[0], hidden)
# print(output)
# print(output.shape)  torch.Size([1, 18])
# print(next_hidden.shape)  torch.Size([1, 128])

def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    # 因为输出是torch.Size([1, 18])的，因此是提取第一维度的最大值。
    # top_n, top_i    值   下标
    category_i = top_i[0].item()  # 提取张量的值
    return all_categories[category_i], category_i

print(categoryFromOutput(output))
# ('Greek', 7)
'''

# ======================================= 建立数据集的方法 ======================================= #
def randomChoice(l):
    return l[random.randint(0, len(l) - 1)]

def randomTrainingExample():
    category = randomChoice(all_categories)
    line = randomChoice(category_lines[category])
    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)
    line_tensor = lineToTensor(line)
    return category, line, category_tensor, line_tensor
    # 是哪一类，  该类中的一个句子， 变成张量， 句子变成张量。

for i in range(10):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    # print('category =', category, '/ line =', line)
    # Arabic Essa   第一个是语言的一种，第二个是该类语言的一个词。
    # print(category_tensor.shape)  torch.Size([1])
    # print(line_tensor.shape)  torch.Size([4, 1, 57])   因为这个单词有4个词
    # ***************************************
    # 发现和LSTM和GRU一样的一点，其实第二个维度可以是批量的大小，设置为1, 那么一次只送入一个。
    # ***************************************
# ======================================= 训练 ======================================= #
criterion = nn.NLLLoss()
learning_rate = 0.005

def train(category_tensor, line_tensor):
    hidden = rnn.initHidden()
    rnn.zero_grad()
    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)
    loss = criterion(output, category_tensor)
    loss.backward()
    for p in rnn.parameters():
        p.data.add_(p.grad.data, alpha=-learning_rate)
    return output, loss.item()

n_iters = 100000
print_every = 5000
plot_every = 1000
current_loss = 0
all_losses = []

def timeSince(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

start = time.time()

def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_categories[category_i], category_i

for iter in range(1, n_iters + 1):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    output, loss = train(category_tensor, line_tensor)
    current_loss += loss

    # Print iter number, loss, name and guess
    if iter % print_every == 0:
        guess, guess_i = categoryFromOutput(output)
        correct = '✓' if guess == category else '✗ (%s)' % category
        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))

    # Add current loss avg to list of losses
    if iter % plot_every == 0:
        all_losses.append(current_loss / plot_every)
        current_loss = 0

# ======================================= 图像绘制 ======================================= #
plt.figure()
plt.plot(all_losses)

# Keep track of correct guesses in a confusion matrix
confusion = torch.zeros(n_categories, n_categories)
n_confusion = 10000

# Just return an output given a line
def evaluate(line_tensor):
    hidden = rnn.initHidden()

    for i in range(line_tensor.size()[0]):
        output, hidden = rnn(line_tensor[i], hidden)

    return output

# Go through a bunch of examples and record which are correctly guessed
for i in range(n_confusion):
    category, line, category_tensor, line_tensor = randomTrainingExample()
    output = evaluate(line_tensor)
    guess, guess_i = categoryFromOutput(output)
    category_i = all_categories.index(category)
    confusion[category_i][guess_i] += 1
    # confusion[真实值][估计值] += 1

# Normalize by dividing every row by its sum
for i in range(n_categories):
    confusion[i] = confusion[i] / confusion[i].sum()
    # 对每个元素。

# Set up plot
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(confusion.numpy())
fig.colorbar(cax)

# Set up axes
ax.set_xticklabels([''] + all_categories, rotation=90)
ax.set_yticklabels([''] + all_categories)

# Force label at every tick
ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

# sphinx_gallery_thumbnail_number = 2
plt.show()

def predict(input_line, n_predictions=3):
    print('\n> %s' % input_line)
    with torch.no_grad():
        output = evaluate(lineToTensor(input_line))

        # Get top N categories
        topv, topi = output.topk(n_predictions, 1, True)
        predictions = []

        for i in range(n_predictions):
            value = topv[0][i].item()
            category_index = topi[0][i].item()
            print('(%.2f) %s' % (value, all_categories[category_index]))
            predictions.append([value, all_categories[category_index]])

predict('Dovesky')
predict('Jackson')
predict('Satoshi')
