Bert可以用来进行分类、标注、匹配等多种自然语言理解任务。这里需要用Bert重新实现上述三个任务中的任意一个。（难度：任务一 < 任务三 < 任务二）

建议使用的框架：Huggingface，https://github.com/huggingface/transformers

分享两篇博文：
①transformer代码实现，充分了解transformer模型框架：https://www.yingzq.com/2019/11/18/the-annotated-transformer/
②bert源代码解析，帮助了解bert模型框架：
http://xtf615.com/2020/07/05/transformers/

  师兄的论文阅读顺序：
 《Attention is All You Need》 
 《Deep Contextualized Word Representations》 
 《Improving Language Understanding by Generative Pre-Training》 
 《BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding》

直接调用写好的bert包：bert-base-uncased下载：
直接执行命令即可。
pip install pytorch_transformers


bert-base-uncased下载及使用：网上资料太难找了，浪费了我好长时间。
https://zhuanlan.zhihu.com/p/106880488
https://www.cnblogs.com/lian1995/p/11947522.html
https://blog.csdn.net/season77us/article/details/104311195?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.not_use_machine_learn_pai&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-2.not_use_machine_learn_pai



bert的输入及输出，以及使用bert解决具体问题的思路：

https://www.cnblogs.com/gczr/p/11785930.html
https://cloud.tencent.com/developer/article/1389555
