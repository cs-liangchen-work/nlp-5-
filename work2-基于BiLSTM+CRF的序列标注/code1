'''
感谢自己没有放弃将官网的pytorch改为批量的输入。虽然大部分的修改都是用加一层for循环来水的。

存在以下疑问：

问题1：
为什么我的loss在反向传播的时候：RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn ？
就是告诉我这个变量不能进行误差的反向传播，当我加了一个loss.requires_grad_(True)。表示让这个变量可以进行了。为什么之前没有出现过？
可能的原因，自己在过程中定义了list形式的。

问题2：
loss.backward()的怎么知道哪些参数是要进行更新的？
我想到了在学官网的梯度传播的时候，有说可以看整个计算过程，因此，pytorch架构可能可以列出从输入到计算loss整个过程的计算图，来更新参数？
但是为什么定义中的抓太转移概率矩阵会进行更新，我仔细看了看，可能其他的self.XXX的内容也会更新，只是下次重新传入了。
不用forward应该也是可以实现反向传播的。

问题3：
在之后的代码改进的时候，我想使用accuracy_score计算正确值的占比，这样我需要计算预测的标签，在计算预测标签的时候，我也需要用到LSTM
的参数，和crf的参数，因为在用维特比算法的时候需要同时用到发射和转移，那我为什么不直接用交叉熵呢？



'''




import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.optim as optim
import argparse
import gensim
from torchtext.data import Field, Example, Dataset, BucketIterator, Iterator
from torch import nn
import torch.nn.functional as F
from sklearn.metrics import accuracy_score
import datetime
from collections import Counter
import torch
import argparse
from gensim.models import Word2Vec
import datetime
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
import os
# 设计随机初始化种子
torch.manual_seed(1)

'''
首先进行说明，损失函数是自己定义的，因此在进行训练的时候会有一些差异。
'''

# ======================================= 一些初始化参数 ======================================= #
parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=10)
parser.add_argument('--num_epoch', type=int, default=5)
parser.add_argument('--lr', type=float, default=0.0004)
parser.add_argument('--weight_decay', type=float, default=0)
parser.add_argument('--ckp', type=str, default='model_AW.pt')
parser.add_argument('--max_acc', type=float, default=0.01)
parser.add_argument('--num_layers', type=int, default=1)  # 双向LSTM的层数。
args = parser.parse_args()

device = torch.device('cpu')
batch_size = args.batch_size
max_acc = args.max_acc
ckp = args.ckp
weight_decay = args.weight_decay
lr = args.lr
num_epoch = args.num_epoch

# ======================================= 加载数据集并处理 ======================================= #
#读入训练集
with open('./CoNLL2003_NER/mini_train.txt', encoding='utf-8') as ftrain_feature:
    train_feature_line = [line.strip() for line in ftrain_feature.readlines()]
with open('./CoNLL2003_NER/mini_train_label.txt', encoding='utf-8') as ftrain_label:
    train_label_line = [line.strip() for line in ftrain_label.readlines()]
#读入验证集
with open('./CoNLL2003_NER/mini_test.txt', encoding='utf-8') as ftest_feature:
    test_feature_line = [line.strip() for line in ftest_feature.readlines()]
with open('./CoNLL2003_NER/mini_test_label.txt', encoding='utf-8') as ftest_label:
    test_label_line = [line.strip() for line in ftest_label.readlines()]

#转换大小写并用split分隔开存入列表
train_feature_line = [line.lower().split(" ") for line in train_feature_line]
train_label_line = [line.split(" ") for line in train_label_line]
test_feature_line = [line.lower().split(" ") for line in test_feature_line]
test_label_line = [line.split(" ") for line in test_label_line]

#获得单词字典
w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./glove2word2vec.6B.50d.txt', binary=False, encoding='utf-8')
word2id = dict(zip(w2v_model.wv.index2word,range(len(w2v_model.wv.index2word))))                              # word -> id
id2word = {idx:word for idx,word in enumerate(w2v_model.wv.index2word)}                                       # id -> word
unk = word2id['unk']              #UNK:低频词
padding_value = word2id['pad']    #PAD:填充词
#获得标签字典
label2id = {'O': 0, 'B-LOC': 1, 'B-PER': 2, 'B-ORG': 3, 'I-PER': 4, 'I-ORG': 5, 'B-MISC': 6, 'I-LOC': 7, 'I-MISC': 8, '<START>': 9, '<STOP>': 10}
#获得数据和标签序列
train_feature = [[word2id[word] if word in word2id else unk for word in line] for line in train_feature_line]
train_label = [[label2id[word] for word in line] for line in train_label_line]
test_feature = [[word2id[word] if word in word2id else unk for word in line] for line in test_feature_line]
test_label = [[label2id[word] for word in line] for line in test_label_line]

#转成Tensor的形式
train_feature = [torch.Tensor(line).long() for line in train_feature]
train_label = [torch.Tensor(line).long() for line in train_label]
test_feature = [torch.Tensor(line).long() for line in test_feature]
test_label = [torch.Tensor(line).long() for line in test_label]


def get_data(sample_features, sample_labels):
    sample_data = []                                                    #为了能够将data放到DataLoader中
    for i in range(len(sample_features)):
        temp = []
        temp.append(sample_features[i])
        temp.append(sample_labels[i])
        sample_data.append(temp)
    return sample_data


def collate_fn(sample_data):
    sample_data.sort(key=lambda data: len(data[0]), reverse=True)                          #倒序排序
    sample_features, sample_labels = [], []
    for data in sample_data:
        sample_features.append(data[0])
        sample_labels.append(data[1])
    data_length = [len(data[0]) for data in sample_data]                                   #取出所有data的长度
    sample_features = torch.nn.utils.rnn.pad_sequence(sample_features, batch_first=True, padding_value=padding_value)
    sample_labels = torch.nn.utils.rnn.pad_sequence(sample_labels, batch_first=True, padding_value=0)
    return sample_features, sample_labels, data_length


train_data = get_data(train_feature, train_label)
test_data = get_data(test_feature, test_label)

#处理非定长序列
train_dataloader = torch.utils.data.DataLoader(train_data, batch_size, collate_fn=collate_fn, shuffle=True)
test_dataloader = torch.utils.data.DataLoader(test_data, batch_size, collate_fn=collate_fn, shuffle=True)

'''
for epoch in range(args.num_epoch):
    for data_x, data_y, batch_seq_len in train_dataloader:
        print(data_x.shape)
        print(data_y.shape)
        print(batch_seq_len)
        torch.Size([10, 37])
        torch.Size([10, 37])
        [37, 34, 34, 33, 32, 28, 26, 21, 11, 10]   这是把每一个的具体长度都输出了。
'''

# ======================================= 模型定义 ======================================= #
def argmax(vec):
    # return the argmax as a python int
    # 返回vec的dim为1维度上的最大索引。
    _, idx = torch.max(vec, 1)
    # 回忆一下max函数，返回的第一个是最大值的值，第二个是最大值所在的索引。
    return idx.item()


def prepare_sequence(seq, to_ix):
    # 将句子转化为id。
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long)

def log_sum_exp(vec):
    # print(vec.shape)
    # torch.Size([1, 5])
    max_score = vec[0, argmax(vec)]
    # 不是分割，而是去取下标为0，argmax的最大值，   即max_score取出了vec中的最大值。
    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])
    # expand 扩展成一样的，比如最大值如果是5，  则 tensor([[5, 5, 5, 5, 5]])
    a = max_score + \
        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))
    # print(a)  tensor(5.7115, grad_fn=<AddBackward0>)    是一个具体的数字。
    return a

class BiLSTM_CRF(nn.Module):
    def __init__(self, embedding_matrix, vocab_size, hidden_dim, label2id):
        super(BiLSTM_CRF, self).__init__()
        self.embedding_dim = hidden_dim  # word embedding dim
        self.hidden_dim = hidden_dim    # Bi-LSTM hidden dim
        self.vocab_size = vocab_size    # 字典中有多少字。
        self.label_size = len(label2id)
        self.label2id = label2id

        # 词嵌入：
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))
        self.embedding.weight.requires_grad = False

        # LSTM
        self.lstm = nn.LSTM(hidden_dim, hidden_dim // 2, batch_first=True, num_layers=args.num_layers, bidirectional=True)

        self.hidden2tag = nn.Linear(hidden_dim, self.label_size)

        self.hidden = self.init_hidden()

        # CRF 的     这些是要训练的参数，为什么会训练这些呢？
        self.transitions = nn.Parameter(torch.randn(self.label_size, self.label_size))
        self.transitions.data[label2id[START_TAG], :] = -10000
        self.transitions.data[:, label2id[STOP_TAG]] = -10000

    def init_hidden(self):
        return (torch.randn(args.num_layers * 2, batch_size, self.hidden_dim // 2),
                torch.randn(args.num_layers * 2, batch_size, self.hidden_dim // 2))

    def _get_lstm_features(self, inputs):
        self.hidden = self.init_hidden()
        # 输入 【10,40】   批量大小（句子的个数）为10，每个具体的句子中词的个数为40
        out = self.embedding(inputs)  # [10, 40, 50]
        out, self.hidden = self.lstm(out, self.hidden)  # torch.Size([10, 40, 50])
        out = self.hidden2tag(out)  # torch.Size([10, 40, 11])
        return out

    def _score_sentence(self, feats, tags):
        # 计算给定tag序列（真实标记情况）的分数，

        # feats  发射分数   ：直接从BILSTM的结果过来。[10, 40, 11]
        # tags   真实的标签 ： [10, 40]
        # 标签的开头加上start_tag
        tags = torch.cat((torch.tensor([[self.label2id[START_TAG]]] * batch_size, dtype=torch.long), tags), dim=1)

        # 第一个tag 不涉及 发射分数。
        scores = []
        for j, feats2 in enumerate(feats):
            score = 0.0
            for i, feat in enumerate(feats2):
                score = score + feat[tags[j][i + 1]] + self.transitions[tags[j][i + 1], tags[j][i]]
            score = score + self.transitions[self.label2id[STOP_TAG], tags[j][-1]]
            scores.append(score.item())
        scores = torch.tensor(scores)
        # print(scores.shape)  torch.Size([10])     批量中每个句子的分数。
        return scores

    def _forward_alg(self, feats2):
        # 通过前向算法计算所有可能路径的分数总和。
        alphas = []
        for feats in feats2:
            init_alphas = torch.full((1, self.label_size), -10000.)
            init_alphas[0][self.label2id[START_TAG]] = 0.

            forward_var = init_alphas

            for feat in feats:
                alphas_t = []  # The forward tensors at this timestep
                for next_tag in range(self.label_size):
                    emit_score = feat[next_tag].view(
                        1, -1).expand(1, self.label_size)
                    trans_score = self.transitions[next_tag].view(1, -1)
                    next_tag_var = forward_var + trans_score + emit_score
                    alphas_t.append(log_sum_exp(next_tag_var).view(1))
                forward_var = torch.cat(alphas_t).view(1, -1)
            terminal_var = forward_var + self.transitions[self.label2id[STOP_TAG]]
            alpha = log_sum_exp(terminal_var)
            alphas.append(alpha)
        alphas = torch.tensor(alphas)
        # print(alphas.shape)  torch.Size([10])
        return alphas

    def neg_log_likelihood(self, sentence, tags):
        # crf损失函数。    需要真实路径的分数 和 所有路径的总分数。
        # 真实路径的分数应该是所有路径中分数最高的。
        # -[ log（ 真实路径的分数 ）/ log（ 所有可能路径的分数 ）]
        feats = self._get_lstm_features(sentence)
        forward_score = self._forward_alg(feats)  # 通过前向算法计算所有可能路径的分数总和。
        gold_score = self._score_sentence(feats, tags)  # 计算给定的该标签的可能的分数。
        return torch.sum(forward_score - gold_score, dim=0)/10

    def forward(self, inputs, tags):
        feats = self._get_lstm_features(inputs)

        return a


START_TAG = "<START>"
STOP_TAG = "<STOP>"
tag_num = 10

embedding_matrix = w2v_model.wv.vectors
model = BiLSTM_CRF(embedding_matrix, embedding_matrix.shape[0], embedding_matrix.shape[1], label2id)
# 词向量矩阵， 词的数量，词向量的深度，label2id

optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)
for epoch in range(args.num_epoch):
    for data_x, data_y, batch_seq_len in train_dataloader:
        model.zero_grad()
        loss = model.neg_log_likelihood(data_x, data_y)
        print('epoch %d, train_loss %f' % (
            epoch + 1, loss))
        loss.requires_grad_(True)
        loss.backward()
        optimizer.step()
