import torch
import argparse
import gensim
from torchtext.data import Field, Example, Dataset, BucketIterator, Iterator
from torch import nn
import torch.nn.functional as F
from sklearn.metrics import accuracy_score
import datetime
# ======================================= 一些初始化参数 ======================================= #
parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=10)
parser.add_argument('--num_epoch', type=int, default=5)
parser.add_argument('--lr', type=float, default=0.0004)
parser.add_argument('--weight_decay', type=float, default=0)
parser.add_argument('--ckp', type=str, default='model_AW.pt')
parser.add_argument('--max_acc', type=float, default=0.01)
args = parser.parse_args()

device = torch.device('cpu')
batch_size = args.batch_size
max_acc = args.max_acc
ckp = args.ckp
weight_decay = args.weight_decay
lr = args.lr
num_epoch = args.num_epoch

# ======================================= 加载数据集并处理 ======================================= #
# 训练集
with open('./snli_1.0/mini_train_sentence1_split.txt', 'r', encoding='utf-8') as f:
    train_feature1_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_train_sentence2_split.txt', 'r', encoding='utf-8') as f:
    train_feature2_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_train_gold_label.txt', 'r', encoding='utf-8') as f:
    train_label_line = [line.strip() for line in f.readlines()]
# 验证集
with open('./snli_1.0/mini_dev_sentence1_split.txt', 'r', encoding='utf-8') as f:
    dev_feature1_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_dev_sentence2_split.txt', 'r', encoding='utf-8') as f:
    dev_feature2_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_dev_gold_label.txt', 'r', encoding='utf-8') as f:
    dev_label_line = [line.strip() for line in f.readlines()]
# 测试集
with open('./snli_1.0/mini_test_sentence1_split.txt', 'r', encoding='utf-8') as f:
    test_feature1_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_test_sentence2_split.txt', 'r', encoding='utf-8') as f:
    test_feature2_line = [line.strip() for line in f.readlines()]
with open('./snli_1.0/mini_test_gold_label.txt', 'r', encoding='utf-8') as f:
    test_label_line = [line.strip() for line in f.readlines()]

# split
train_feature1_line = [line.split(" ") for line in train_feature1_line]
train_feature2_line = [line.split(" ") for line in train_feature2_line]
dev_feature1_line = [line.split(" ") for line in dev_feature1_line]
dev_feature2_line = [line.split(" ") for line in dev_feature2_line]
test_feature1_line = [line.split(" ") for line in test_feature1_line]
test_feature2_line = [line.split(" ") for line in test_feature2_line]

# 加载训练好的词向量：
# 使用的是glove，但是glove中只有unk，pad不敢确定。
w2v_model = gensim.models.KeyedVectors.load_word2vec_format('glove2word2vec.6B.50d.txt', binary=False, encoding='utf-8')
print('loading word2vec......')
word2id = dict(zip(w2v_model.wv.index2word, range(len(w2v_model.wv.index2word))))
id2word = {idx: word for idx, word in enumerate(w2v_model.wv.index2word)}
feature_pad = word2id['pad']  # 10109
feature_unk = word2id['unk']  # 201534
label2id = {'neutral': 0, 'entailment': 1, 'contradiction': 2}  # 可以确定模型最后一层
label_pad = 0

# token转化为下标：
train_feature1 = [[word2id[word] if word in word2id else feature_unk for word in words] for words in train_feature1_line]
train_feature2 = [[word2id[word] if word in word2id else feature_unk for word in line] for line in train_feature2_line]
train_label = [[label2id[word] if word in label2id else label_pad] for word in train_label_line]
dev_feature1 = [[word2id[word] if word in word2id else feature_unk for word in line] for line in dev_feature1_line]
dev_feature2 = [[word2id[word] if word in word2id else feature_unk for word in line] for line in dev_feature2_line]
dev_label = [[label2id[word] if word in label2id else label_pad] for word in dev_label_line]
test_feature1 = [[word2id[word] if word in word2id else feature_unk for word in line] for line in test_feature1_line]
test_feature2 = [[word2id[word] if word in word2id else feature_unk for word in line] for line in test_feature2_line]
test_label = [[label2id[word] if word in label2id else label_pad] for word in test_label_line]


# ======================================= torchText ======================================= #
# field，规定格式：
sentence1_field = Field(sequential=True, use_vocab=False, batch_first=True, fix_length=25, pad_token=feature_pad)
sentence2_field = Field(sequential=True, use_vocab=False, batch_first=True, fix_length=25, pad_token=feature_pad)
label_field = Field(sequential=False, use_vocab=False)

fields = [('sentence1', sentence1_field), ('sentence2', sentence2_field), ('label', label_field)]

# Example
train_examples = []
for index in range(len(train_label)):
    train_examples.append(Example.fromlist([train_feature1[index], train_feature2[index], train_label[index]], fields))

# Dataset
train_set = Dataset(train_examples, fields)

# 迭代器：
train_iter = BucketIterator(train_set, batch_size=batch_size, device=device, shuffle=True)


# 验证集的Iterator
dev_examples = []
for index in range(len(dev_label)):
    dev_examples.append(Example.fromlist([dev_feature1[index], dev_feature2[index], dev_label[index]], fields))
dev_set = Dataset(dev_examples, fields)
dev_iter = Iterator(dev_set, batch_size=batch_size, device=device, train=False, shuffle=False, sort=False)

# 测试集的Iterator
test_examples = []
for index in range(len(test_label)):
    test_examples.append(Example.fromlist([test_feature1[index], test_feature2[index], test_label[index]], fields))
test_set = Dataset(test_examples, fields)
test_iter = Iterator(test_set, batch_size=batch_size, device=device, train=False, shuffle=False, sort=False)

# ======================================= ESIM模型核心部分 ======================================= #
class ESIM(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size, embedding_vector):
        # 总词汇的个数， 嵌入的深度， 3分类问题的3， 具体的每一个词的词向量组成
        super(ESIM, self).__init__()
        self.hidden_size = hidden_size

        self.embedding = torch.nn.Embedding(input_size, hidden_size)
        self.embedding.weight.data.copy_(torch.from_numpy(embedding_vector))
        self.embedding.weight.requires_grad = False

        self.bilstm1 = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, dropout=0.5, bidirectional=True)
        # （ 50, 50 ）
        self.bilstm2 = torch.nn.LSTM(input_size=hidden_size * 8, hidden_size=hidden_size, batch_first=True, dropout=0.5, bidirectional=True)
        # （ 400, 50）

        # 最大池化后的操作
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 8, 2),
            nn.Dropout(0.5),
            nn.Linear(2, output_size),  # 三分类问题
            nn.Softmax(dim=-1)  # 分类问题都是对最后一个维度softmax。
        )



    def attention(self, seq1, seq2, mask1, mask2):
        # 注意力矩阵：
        # 第一个维度是批量，两个句子对应的（2, 3）维度相乘才是注意力。
        # n维矩阵相乘，前n-2维度不变且相等，后2个维度满足两个二维矩阵的乘法。
        eik = torch.matmul(seq2, seq1.transpose(1, 2))
        ekj = torch.matmul(seq1, seq2.transpose(1, 2))
        # (10, 25, 25）

        # mask操作：将相似度矩阵中值为1（.的填充id）的那些值全部用-1e9mask掉
        # print(mask1.shape)    torch.Size([10, 25])
        # print(eik.shape)      torch.Size([10, 25, 25])
        eik = eik.masked_fill(mask1.unsqueeze(-1) == 1, -1e9)
        # squeeze()： 压缩，压缩维度，
        # unsqueeze()： 解压，增加维度。
        # mask1的大小需要与eki的大小一致，将mask1中值为1的位置设置为第二个擦书的值。
        # 为了大小一致，进行了维度增加，第三维度的大小发生变化。例子在最下面。
        ekj = ekj.masked_fill(mask2.unsqueeze(-1) == 1, -1e9)
        # 应该是防止填充元素的影响吧！

        # 归一化用于后续加权计算
        eik = F.softmax(eik, dim=-1)
        ekj = F.softmax(ekj, dim=-1)
        # [10, 25, 25]

        # 通过相似度和b的加权和计算出ai，通过相似度和a的加权和计算出bj
        ai = torch.matmul(ekj, seq2)  # torch.Size([10, 25, 25])  *  torch.Size([10, 25, 100])  因为双向 LSTM 50*2 了
        bj = torch.matmul(eik, seq1)
        # torch.Size([10, 25, 100])
        return ai, bj

    def submul(self, x1, x2):
        # 计算差和积
        sub = x1 - x2
        mul = x1 * x2
        return torch.cat([sub, mul], -1)

    def pooling(self, x):
        # 通过平均池和最大池获得固定长度的向量，并拼接送至最终分类器
        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)
        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)
        # 对文本进行池化，是对token进行池化，所以要交换。
        # x.size(1)，设置步长，即只取最大值了（同一个句子中，每个token同一位置词向量的最大值），
        # 因为池化不改变维度，把最后一个维度squeeze掉。

        # torch.Size([10, 100])
        return torch.cat([p1, p2], 1)

    def forward(self, seq1, seq2, mask1, mask2):
        # ==================== embedding ==================== #
        seq1 = self.embedding(seq1)
        seq2 = self.embedding(seq2)
        # （10, 25，50）
        # ==================== bilstm  ==================== #
        bi1_1, _ = self.bilstm1(seq1)
        bi1_2, _ = self.bilstm1(seq2)
        #（10，25，100）
        # ==================== attention ==================== #
        ai, bj = self.attention(bi1_1, bi1_2, mask1, mask2)
        #（10，25，100）    注意力机制，不改变维度。

        # 计算差和积然后和原向量合并，对应论文中 ma=[-a;~a;-a-~a;-a*~a] 和 mb=[-b;~b;-b-~b;-b*~b]
        ma = torch.cat([bi1_1, ai, self.submul(bi1_1, ai)], -1)
        mb = torch.cat([bi1_2, bj, self.submul(bi1_2, bj)], -1)
        # （10，25，100 + 100 + 100 + 100 = 8 * 50 = 400）

        # ==================== bilstm ==================== #
        bi2_1, _ = self.bilstm2(ma)  # （ 400 ， 50 ）
        bi2_2, _ = self.bilstm2(mb)
        # （ 10，25，800 ）

        # ==================== fc ==================== #
        output_1 = self.pooling(bi2_1)
        # print(bi2_1.shape) torch.Size([10, 25, 100])
        # print(output_1.shape) torch.Size([10, 200])   最大池化和平均池化拼接了。
        output_2 = self.pooling(bi2_2)
        output = torch.cat([output_1, output_2], -1)
        # torch.Size([10, 400])
        output = self.fc(output)
        return output




# ======================================= ESIM模型核心部分 ======================================= #

def dev_evaluate(device, net, dev_iter, max_acc, ckp):
    dev_l, n = 0.0, 0
    out_epoch, label_epoch = [], []
    loss_func = torch.nn.CrossEntropyLoss()
    net.eval()
    with torch.no_grad():
        for batch in dev_iter:
            seq1 = batch.sentence1
            seq2 = batch.sentence2
            label = batch.label
            mask1 = (seq1 == 1)
            mask2 = (seq2 == 1)
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))

            loss = loss_func(out, label.squeeze(-1))

            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            # 测试集评价指标
            out_epoch.extend(prediction)
            label_epoch.extend(label)
            dev_l += loss.item()
            n += 1

        acc = accuracy_score(label_epoch, out_epoch)
        if acc > max_acc:
            max_acc = acc
            torch.save(net.state_dict(), ckp)
            print("save model......")

    return dev_l / n, acc, max_acc


def test_evaluate(device, net, test_iter):
    test_l, n = 0.0, 0
    out_epoch, label_epoch = [], []
    loss_func = torch.nn.CrossEntropyLoss()
    net.eval()
    with torch.no_grad():
        for batch in test_iter:
            seq1 = batch.sentence1
            seq2 = batch.sentence2
            label = batch.label
            mask1 = (seq1 == 1)
            mask2 = (seq2 == 1)
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))

            loss = loss_func(out, label.squeeze(-1))

            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            # 测试集评价指标
            out_epoch.extend(prediction)
            label_epoch.extend(label)
            test_l += loss.item()
            n += 1

        acc = accuracy_score(label_epoch, out_epoch)

    return test_l / n, acc


def training(device, w2v_model, train_iter, dev_iter, test_iter, batch_size, num_epoch, lr, weight_decay, ckp, max_acc):
    embedding_matrix = w2v_model.wv.vectors
    input_size, hidden_size = embedding_matrix.shape[0], embedding_matrix.shape[1]
    loss_func = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数 ，
    net = ESIM(input_size, hidden_size, 3, embedding_matrix).to(device)
    # 4 ：是因为本问题是一个四分类问题。
    # net.load_state_dict(torch.load(ckp, map_location='cpu'))
    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)

    for epoch in range(num_epoch):
        net.train()
        train_l, n = 0.0, 0
        start = datetime.datetime.now()
        out_epoch, label_epoch = [], []
        for batch in train_iter:
            seq1 = batch.sentence1  # torch.Size([10, 25])  批量为10，每个句子25个单词。
            seq2 = batch.sentence2  # torch.Size([10, 25])
            label = batch.label  # torch.Size([10, 1])   每个句子对应的标签。
            mask1 = (seq1 == 1)  # 对逐个元素进行操作。torch.Size([10, 25])
            # 为什么是1呢？
            mask2 = (seq2 == 1)  # 对逐个元素进行操作。torch.Size([10, 25])
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))
            # print(out.shape)   [10, 3]   （批量，三分类问题）
            loss = loss_func(out, label.squeeze(-1))
            optimizer.zero_grad()
            loss.backward()  # 反向传播，计算当前梯度；
            optimizer.step()  # 根据梯度更新网络参数。
            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            out_epoch.extend(prediction)
            label_epoch.extend(label)

            train_l += loss.item()
            # loss.item()  用于将一个零维张量转换成浮点数，比如计算loss，accuracy的值
            n += 1

        train_acc = accuracy_score(label_epoch, out_epoch)
        # accuracy_score函数。返回正确分类的比例。
        dev_loss, dev_acc, max_acc = dev_evaluate(device, net, dev_iter, max_acc, ckp)
        test_loss, test_acc = test_evaluate(device, net, test_iter)
        end = datetime.datetime.now()
        print('epoch %d, train_acc %f, dev_acc %f, test_acc %f, max_acc %f, time %s' % (
        epoch + 1, train_acc, dev_acc, test_acc, max_acc, end - start))


training(device, w2v_model, train_iter, dev_iter, test_iter, args.batch_size, args.num_epoch, args.lr,
         args.weight_decay, args.ckp, args.max_acc)




'''
# (2,3,4)
eik = torch.tensor([[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]],

                    [[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]]
                    ])
print(eik.shape)

# （2,3）
mask1 = torch.tensor([[1, 0, 1],

                       [1, 0, 1]
                        ])

eik = eik.masked_fill(mask1.unsqueeze(-1) == 1, -1e9)
print(eik)

输出：

torch.Size([2, 3, 4])
tensor([[[-1000000000, -1000000000, -1000000000, -1000000000],
         [          1,           1,           0,           0],
         [-1000000000, -1000000000, -1000000000, -1000000000]],

        [[-1000000000, -1000000000, -1000000000, -1000000000],
         [          1,           1,           0,           0],
         [-1000000000, -1000000000, -1000000000, -1000000000]]])
'''
