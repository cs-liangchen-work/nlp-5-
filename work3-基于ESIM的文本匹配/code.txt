import torch
import argparse
import gensim
from torchtext.data import Field, Example, Dataset, BucketIterator, Iterator
from torch import nn
import torch.nn.functional as F
from sklearn.metrics import accuracy_score
import datetime
# ======================================= 一些初始化参数 ======================================= #
parser = argparse.ArgumentParser()
parser.add_argument('--batch_size', type=int, default=32)
parser.add_argument('--num_epoch', type=int, default=50)
parser.add_argument('--lr', type=float, default=0.0004)
parser.add_argument('--weight_decay', type=float, default=0)
parser.add_argument('--ckp', type=str, default='model_3.pt')
parser.add_argument('--max_acc', type=float, default=0.5)
args = parser.parse_args()

device = torch.device('cpu')
batch_size = 32
max_acc = 0.5
ckp = 'model_3.pt'  # 存放的是该模型的参数
weight_decay = 0    # 权重衰减。
lr = 0.0004
num_epoch = 50
# ======================================= 加载数据集并处理 ======================================= #

# 读入训练集
with open('./snli_1.0/train_sentence1_split.txt', 'r', encoding='utf-8') as ftrain_feature1:
    train_feature1_line = [line.strip() for line in ftrain_feature1.readlines()]
with open('./snli_1.0/train_sentence2_split.txt', 'r', encoding='utf-8') as ftrain_feature2:
    train_feature2_line = [line.strip() for line in ftrain_feature2.readlines()]
with open('./snli_1.0/train_gold_label.txt', 'r', encoding='utf-8') as ftrain_label:
    train_label_line = [line.strip() for line in ftrain_label.readlines()]
# 读入验证集
with open('./snli_1.0/dev_sentence1_split.txt', 'r', encoding='utf-8') as fdev_feature1:
    dev_feature1_line = [line.strip() for line in fdev_feature1.readlines()]
with open('./snli_1.0/dev_sentence2_split.txt', 'r', encoding='utf-8') as fdev_feature2:
    dev_feature2_line = [line.strip() for line in fdev_feature2.readlines()]
with open('./snli_1.0/dev_gold_label.txt', 'r', encoding='utf-8') as fdev_label:
    dev_label_line = [line.strip() for line in fdev_label.readlines()]
# 读入测试集
with open('./snli_1.0/test_sentence1_split.txt', 'r', encoding='utf-8') as ftest_feature1:
    test_feature1_line = [line.strip() for line in ftest_feature1.readlines()]
with open('./snli_1.0/test_sentence2_split.txt', 'r', encoding='utf-8') as ftest_feature2:
    test_feature2_line = [line.strip() for line in ftest_feature2.readlines()]
with open('./snli_1.0/test_gold_label.txt', 'r', encoding='utf-8') as ftest_label:
    test_label_line = [line.strip() for line in ftest_label.readlines()]

# 用split分隔开存入列表   [ [第一个句子单词组成的列表], [第二个句子。。。], [第三个句子。。。] ]
train_feature1_line = [line.split(" ") for line in train_feature1_line]
train_feature2_line = [line.split(" ") for line in train_feature2_line]
dev_feature1_line = [line.split(" ") for line in dev_feature1_line]
dev_feature2_line = [line.split(" ") for line in dev_feature2_line]
test_feature1_line = [line.split(" ") for line in test_feature1_line]
test_feature2_line = [line.split(" ") for line in test_feature2_line]

# 获得单词字典
# model_word2vec = Word2Vec(train_feature1_line+train_feature2_line, sg=1, min_count=1, size=128, window=5)
# model_word2vec.save('word2vec_model.txt')
w2v_model = gensim.models.KeyedVectors.load_word2vec_format('word2vec.txt', binary=False, encoding='utf-8')
print('loading word2vec.txt......')
word2id = dict(zip(w2v_model.wv.index2word, range(len(w2v_model.wv.index2word))))  # word -> id
id2word = {idx: word for idx, word in enumerate(w2v_model.wv.index2word)}  # id -> word
feature_pad = 0  # PAD:填充词
label2id = {'neutral': 0, 'entailment': 1, 'contradiction': 2, '-': 3}
label_pad = 0

# 获得数据和标签序列     即将单词和标签都转化成了对应的id下标。
train_feature1 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in train_feature1_line]
train_feature2 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in train_feature2_line]
train_label = [[label2id[word] if word in label2id else label_pad] for word in train_label_line]
dev_feature1 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in dev_feature1_line]
dev_feature2 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in dev_feature2_line]
dev_label = [[label2id[word] if word in label2id else label_pad] for word in dev_label_line]
test_feature1 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in test_feature1_line]
test_feature2 = [[word2id[word] if word in word2id else feature_pad for word in line] for line in test_feature2_line]
test_label = [[label2id[word] if word in label2id else label_pad] for word in test_label_line]

# ======================================= 使用torchText对文本进行处理。 ======================================= #
# Field 对象： 这个对象包含了我们打算如何预处理文本数据的信息。 她就像一个说明书。
# squential：数据是否为序列数据，默认为Ture。如果为False，则不能使用分词。
# use_vocab：是否使用词典，默认为True。如果为False，那么输入的数据类型必须是数值类型(即使用vocab转换后的)。
# batch_first：batch作为第一个维度；
# fix_length：所有样本的长度，不够则使用pad_token补全。默认为None，表示灵活长度。
# pad_token：用于补全的字符，默认为<pad>。
sentence1_field = Field(sequential=True, use_vocab=False, batch_first=True, fix_length=50, pad_token=feature_pad)
sentence2_field = Field(sequential=True, use_vocab=False, batch_first=True, fix_length=50, pad_token=feature_pad)
label_field = Field(sequential=False, use_vocab=False)
# 使用上述定义的结构。
fields = [('sentence1', sentence1_field), ('sentence2', sentence2_field), ('label', label_field)]

# 获得训练集的Iterator
train_examples = []
for index in range(len(train_label)):
    train_examples.append(Example.fromlist([train_feature1[index], train_feature2[index], train_label[index]], fields))
    # 自己瞎理解。
    # Example.fromlist 两个参数， 第二个参数是定义的结构，第一个参数则是与第二个参数定义的结构对应的元素组成的列表，
    # 列表中的  第一个是 前提的第一个句子，第二个是 推理的第一个句子，第三个是第一个标签。
train_set = Dataset(train_examples, fields)
# 成数据集了
train_iter = BucketIterator(train_set, batch_size=batch_size, device=device, shuffle=True)
# 迭代器。
# 在这里用到了batch_size，决定一次喂入模型的批量。

# 获得验证集的Iterator
dev_examples = []
for index in range(len(dev_label)):
    dev_examples.append(Example.fromlist([dev_feature1[index], dev_feature2[index], dev_label[index]], fields))
dev_set = Dataset(dev_examples, fields)
dev_iter = Iterator(dev_set, batch_size=batch_size, device=device, train=False, shuffle=False, sort=False)

# 获得测试集的Iterator
test_examples = []
for index in range(len(test_label)):
    test_examples.append(Example.fromlist([test_feature1[index], test_feature2[index], test_label[index]], fields))
test_set = Dataset(test_examples, fields)
test_iter = Iterator(test_set, batch_size=batch_size, device=device, train=False, shuffle=False, sort=False)

# ======================================= ESIM模型核心部分 ======================================= #
class ESIM(torch.nn.Module):
    def __init__(self, input_size, hidden_size, output_size, embedding_vector):
        super(ESIM, self).__init__()
        self.hidden_size = hidden_size
        self.embedding = torch.nn.Embedding(input_size, hidden_size)
        # torch.nn.Embedding的两个参数：   第一个，字典中词的个数    第二个，embedding的维度
        self.embedding.weight.data.copy_(torch.from_numpy(embedding_vector))
        # 导入词向量
        self.embedding.weight.requires_grad = False
        # 在反向传播的时候, 不要对这些词向量进行求导更新
        self.bilstm1 = torch.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, batch_first=True, dropout=0.5,
                                     bidirectional=True)
        # LSTM参数的解释：
        # 输入数据的特征维数，通常就是embedding_dim(词向量的维度)
        # LSTM中隐藏层的维度。     有一种说法是  隐藏层的维度必须与词向量的dim相同。
        # 这个要注意，通常我们输入的数据shape=(batch_size,seq_length,embedding_dim),而batch_first默认是False,所以我们的输入数据最好送进LSTM之前将batch_size与seq_length这两个维度调换
        # 使用双向。
        self.bilstm2 = torch.nn.LSTM(input_size=hidden_size * 8, hidden_size=hidden_size, batch_first=True, dropout=0.5,
                                     bidirectional=True)
        # ？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？
        # 疑问，为什么这里是8，拼接之后不应该是4吗？    而且拼接的第四个为什么能相乘。
        self.fc = nn.Sequential(
            # 一个有序的容器，神经网络模块将按照在传入构造器的顺序依次被添加到计算图中执行。
            nn.Linear(hidden_size * 8, 2),
            # in_features由输入张量的形状决定，out_features则决定了输出张量的形状
            nn.Dropout(0.5),
            nn.Linear(2, output_size),  # 因为该问题是四分类问题。
            nn.Softmax(dim=-1)
            # 这里的-1 应该指的是最后一个维度，应该是这种形式，[1, 2, 3]，其内部没有列表，都是纯数字。
            # -1 就如同是倒着的第一个维度。
        )

    def attention(self, seq1, seq2, mask1, mask2):
        # 原来是要先计算出两 个句子 word 之间的相似度，得到2维的相似度矩阵。然后才进行两句话的 local inference。
        # 结合 seq1，seq2 这两句话，互相生成彼此相似性加权后的句子，维度保持不变。
        # 这个应该是利用了技巧，一次性就算出来啦！
        # 首先计算出eij也就是相似度
        eik = torch.matmul(seq2, seq1.transpose(1, 2))
        # transpose  将张量的两个维度进行交换。
        ekj = torch.matmul(seq1, seq2.transpose(1, 2))

        # mask操作：将相似度矩阵中值为1（.的填充id）的那些值全部用-1e9mask掉
        eik = eik.masked_fill(mask1.unsqueeze(-1) == 1, -1e9)
        ekj = ekj.masked_fill(mask2.unsqueeze(-1) == 1, -1e9)
        # 这个应该和自己自注意力矩阵中学的，防止填充的占概率，所以给一个很大的负的偏执项，然后softmax的时候，就变成了0了。

        # 归一化用于后续加权计算
        eik = F.softmax(eik, dim=-1)
        ekj = F.softmax(ekj, dim=-1)

        # 通过相似度和b的加权和计算出ai，通过相似度和a的加权和计算出bj
        ai = torch.matmul(ekj, seq2)
        bj = torch.matmul(eik, seq1)
        return ai, bj

    def submul(self, x1, x2):
        # 计算差和积
        sub = x1 - x2
        mul = x1 * x2
        return torch.cat([sub, mul], -1)

    def pooling(self, x):
        # 通过平均池和最大池获得固定长度的向量，并拼接送至最终分类器
        p1 = F.avg_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)
        p2 = F.max_pool1d(x.transpose(1, 2), x.size(1)).squeeze(-1)
        return torch.cat([p1, p2], 1)

    def forward(self, seq1, seq2, mask1, mask2):
        # ==================== embedding ==================== #
        seq1 = self.embedding(seq1)
        seq2 = self.embedding(seq2)
        # 得到了原始词向量。
        # ==================== bilstm  ==================== #
        bi1_1, _ = self.bilstm1(seq1)
        bi1_2, _ = self.bilstm1(seq2)
        # 得到了由LSTM 有了上下文信息的 词向量。
        # ==================== attention ==================== #
        ai, bj = self.attention(bi1_1, bi1_2, mask1, mask2)
        # 计算差和积然后和原向量合并，对应论文中 ma=[-a;~a;-a-~a;-a*~a] 和 mb=[-b;~b;-b-~b;-b*~b]
        ma = torch.cat([bi1_1, ai, self.submul(bi1_1, ai)], -1)
        mb = torch.cat([bi1_2, bj, self.submul(bi1_2, bj)], -1)

        # ==================== bilstm ==================== #
        bi2_1, _ = self.bilstm2(ma)
        bi2_2, _ = self.bilstm2(mb)

        # ==================== fc ==================== #
        output_1 = self.pooling(bi2_1)
        output_2 = self.pooling(bi2_2)
        output = torch.cat([output_1, output_2], -1)
        output = self.fc(output)

        return output


def dev_evaluate(device, net, dev_iter, max_acc, ckp):
    dev_l, n = 0.0, 0
    out_epoch, label_epoch = [], []
    loss_func = torch.nn.CrossEntropyLoss()
    net.eval()
    with torch.no_grad():
        for batch in dev_iter:
            seq1 = batch.sentence1
            seq2 = batch.sentence2
            label = batch.label
            mask1 = (seq1 == 1)
            mask2 = (seq2 == 1)
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))

            loss = loss_func(out, label.squeeze(-1))

            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            # 测试集评价指标
            out_epoch.extend(prediction)
            label_epoch.extend(label)
            dev_l += loss.item()
            n += 1

        acc = accuracy_score(label_epoch, out_epoch)
        if acc > max_acc:
            max_acc = acc
            torch.save(net.state_dict(), ckp)
            print("save model......")

    return dev_l / n, acc, max_acc


def test_evaluate(device, net, test_iter):
    test_l, n = 0.0, 0
    out_epoch, label_epoch = [], []
    loss_func = torch.nn.CrossEntropyLoss()
    net.eval()
    with torch.no_grad():
        for batch in test_iter:
            seq1 = batch.sentence1
            seq2 = batch.sentence2
            label = batch.label
            mask1 = (seq1 == 1)
            mask2 = (seq2 == 1)
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))

            loss = loss_func(out, label.squeeze(-1))

            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            # 测试集评价指标
            out_epoch.extend(prediction)
            label_epoch.extend(label)
            test_l += loss.item()
            n += 1

        acc = accuracy_score(label_epoch, out_epoch)

    return test_l / n, acc


def training(device, w2v_model, train_iter, dev_iter, test_iter, batch_size, num_epoch, lr, weight_decay, ckp, max_acc):
    embedding_matrix = w2v_model.wv.vectors
    input_size, hidden_size = embedding_matrix.shape[0], embedding_matrix.shape[1]
    loss_func = torch.nn.CrossEntropyLoss()  # 交叉熵损失函数 ，
    net = ESIM(input_size, hidden_size, 4, embedding_matrix).to(device)
    # 4 ：是因为本问题是一个四分类问题。
    # net.load_state_dict(torch.load(ckp, map_location='cpu'))
    optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)

    for epoch in range(num_epoch):
        net.train()
        # 当网络中有 dropout，bn 的时候。训练的要记得 net.train(), 测试 要记得 net.eval()
        train_l, n = 0.0, 0
        start = datetime.datetime.now()
        out_epoch, label_epoch = [], []
        for batch in train_iter:
            seq1 = batch.sentence1
            seq2 = batch.sentence2
            label = batch.label
            mask1 = (seq1 == 1)
            mask2 = (seq2 == 1)
            # 模型定义中的forward的内容
            out = net(seq1.to(device), seq2.to(device), mask1.to(device), mask2.to(device))
            # 具体的维度需要再细细的思考。
            loss = loss_func(out, label.squeeze(-1))
            # 把梯度置零，也就是把loss关于weight的导数变成0  批操作中基本都有这一步骤。
            # 清空过往梯度。
            optimizer.zero_grad()
            loss.backward()  # 反向传播，计算当前梯度；
            optimizer.step()  # 根据梯度更新网络参数。
            # 着实看不太懂。
            prediction = out.argmax(dim=1).data.cpu().numpy().tolist()
            label = label.view(1, -1).squeeze().data.cpu().numpy().tolist()

            out_epoch.extend(prediction)
            label_epoch.extend(label)

            train_l += loss.item()
            # loss.item()  用于将一个零维张量转换成浮点数，比如计算loss，accuracy的值
            n += 1

        train_acc = accuracy_score(label_epoch, out_epoch)
        # accuracy_score函数。返回正确分类的比例。
        dev_loss, dev_acc, max_acc = dev_evaluate(device, net, dev_iter, max_acc, ckp)
        test_loss, test_acc = test_evaluate(device, net, test_iter)
        end = datetime.datetime.now()
        print('epoch %d, train_acc %f, dev_acc %f, test_acc %f, max_acc %f, time %s' % (
        epoch + 1, train_acc, dev_acc, test_acc, max_acc, end - start))

training(device, w2v_model, train_iter, dev_iter, test_iter, args.batch_size, args.num_epoch, args.lr,
         args.weight_decay, args.ckp, args.max_acc)
